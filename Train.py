# -*- coding: utf-8 -*-
"""Training_Evaluation_Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vi0FZjWWgBWkNVQhOFLTpbSKDyppbBn9

### Importing Packages
"""

pip install tensorflow==2.0.0

pip install tensorflow-gpu==2.0.0

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

import math

import itertools

from sklearn.metrics import classification_report, confusion_matrix

"""### Custom Defined Functions"""

def plotImages(images_arr):
    fig, axes = plt.subplots(1, 5, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip( images_arr, axes):
        ax.imshow(img)
    plt.tight_layout()
    plt.show()

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

"""#### Setting TF.Keras backend and TF.Session to be same for later conversion into SavedModel format"""

tf.keras.backend.set_session = tf.compat.v1.Session()

"""### Defining Directory for dataset"""

from google.colab import drive
drive.mount('/content/drive')

base_dir = '/content/drive/My Drive/FYP/DATA/' 
train_dir = os.path.join(base_dir, 'train') 
validation_dir = os.path.join(base_dir, 'val')

"""### Setting up batch size, epochs and image size to be resized into"""

batch_size = 100
epochs = 100
IMG_SHAPE = 224

"""### Downloading the pre-trained model"""

pre_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224,224,3), alpha=1.0,  include_top=False, weights='imagenet')

"""### Setting the layers of pre-trained model to be non trainable for transfer learning"""

for layer in pre_model.layers:
    layer.trainable = False

"""### Setting up Data Generators"""

train_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

validation_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

train_data_gen = train_image_generator.flow_from_directory(
                                                batch_size=batch_size, 
                                                directory=train_dir, 
                                                shuffle=True, 
                                                class_mode='categorical',
                                                target_size=(IMG_SHAPE,IMG_SHAPE))

val_data_gen = validation_image_generator.flow_from_directory(batch_size=50, 
                                                              directory=validation_dir, 
                                                              class_mode='categorical',
                                                              target_size=(IMG_SHAPE,IMG_SHAPE),shuffle=False)

sample_training_images, _ = next(train_data_gen)

plotImages(sample_training_images[:5])

"""### Constructing the model"""

model_fine = tf.keras.models.Sequential()

model_fine.add(pre_model)

model_fine.add(tf.keras.layers.Flatten())

model_fine.add(tf.keras.layers.Dense(64, activation='relu'))
model_fine.add(tf.keras.layers.Dropout(0.4))
model_fine.add(tf.keras.layers.Dense(32, activation='relu'))

model_fine.add(tf.keras.layers.Dense(6, activation='softmax'))

model_fine.summary()

"""### Compiling the model"""

model_fine.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])

"""### Training process"""

history = model_fine.fit_generator(train_data_gen, validation_data=val_data_gen, epochs=50, steps_per_epoch=20, validation_steps=20)

"""### Model evaluated on validation set"""

model_fine.evaluate_generator(val_data_gen)

"""### Plotting the training process"""

acc = history.history['categorical_accuracy']
val_acc = history.history['val_categorical_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(50)

plt.figure(figsize=(20, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

model_fine.save("CashKeras-50-transfer-epoch-drpt-03-lr0001.h5")

pre_model.summary()

"""### Setting the pre-trained model layers from block_15_add to be trainable for finetuning"""

position_layer = pre_model.get_layer('block_15_add')

for layer in pre_model.layers:
  layer.trainable = True

all_layers = pre_model.layers
for i in range(pre_model.layers.index(position_layer)):
    all_layers[i].trainable = False

model_fine.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])

model_fine.summary()

history = model_fine.fit_generator(train_data_gen, validation_data=val_data_gen, epochs=50, steps_per_epoch=20, validation_steps=20)

model_fine.evaluate_generator(val_data_gen)

acc = history.history['categorical_accuracy']
val_acc = history.history['val_categorical_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(50)

plt.figure(figsize=(20, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

model_fine.save("CashKeras-50-finetune-epoch-drpt-03-lr0001.h5")

"""### Plotting the confusion matrix for classification evaluation"""

Y_pred = model_fine.predict_generator(val_data_gen)

y_pred = np.argmax(Y_pred, axis=1)

y_pred.shape

print('Confusion Matrix')
print(confusion_matrix(val_data_gen.classes, y_pred))

cm = confusion_matrix(val_data_gen.classes, y_pred)

print('Classification Report')
target_names = ['fifty', 'fivehundred', 'fivethousand', 'hundred', 'thousand', 'twenty']
print(classification_report(val_data_gen.classes, y_pred, target_names=target_names))

# Compute confusion matrix
cnf_matrix = cm
np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=target_names,
                      title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

pre_model.summary()

"""### Setting the pre-trained model layers from block_13_expand to be trainable for finetuning"""

model_fine = tf.keras.loaded_model('')

position_layer = pre_model.get_layer('block_13_expand')

for layer in pre_model.layers:
  layer.trainable = True

all_layers = pre_model.layers
for i in range(pre_model.layers.index(position_layer)):
    all_layers[i].trainable = False

model_fine.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])

model_fine.summary()

history = model_fine.fit_generator(train_data_gen, validation_data=val_data_gen, epochs=50, steps_per_epoch=20, validation_steps=20)

model_fine.evaluate_generator(val_data_gen)

acc = history.history['categorical_accuracy']
val_acc = history.history['val_categorical_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(50)

plt.figure(figsize=(20, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

model_fine.save("CashKeras-50-finetune_2-epoch-drpt-03-lr0001.h5")

Y_pred = model_fine.predict_generator(val_data_gen)
y_pred = np.argmax(Y_pred, axis=1)

print('Confusion Matrix')
print(confusion_matrix(val_data_gen.classes, y_pred))

cm = confusion_matrix(val_data_gen.classes, y_pred)

print('Classification Report')
target_names = ['fifty', 'fivehundred', 'fivethousand', 'hundred', 'thousand', 'twenty']
print(classification_report(val_data_gen.classes, y_pred, target_names=target_names))

# Compute confusion matrix
cnf_matrix = cm
np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=target_names,
                      title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

pre_model.summary()

"""### Setting all layers of the pre-trained model to be trainable"""

for layer in pre_model.layers:
  layer.trainable = True

model_fine.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0000001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])

model_fine.summary()

history = model_fine.fit_generator(train_data_gen, validation_data=val_data_gen, epochs=50, steps_per_epoch=20, validation_steps=20)

model_fine.evaluate_generator(val_data_gen)

model_fine.save("CashKeras-50-finetune_all-epoch-drpt-03-lr0000001.h5")

Y_pred = model_fine.predict_generator(val_data_gen)
y_pred = np.argmax(Y_pred, axis=1)

print('Confusion Matrix')
print(confusion_matrix(val_data_gen.classes, y_pred))

cm = confusion_matrix(val_data_gen.classes, y_pred)

print('Classification Report')
target_names = ['fifty', 'fivehundred', 'fivethousand', 'hundred', 'thousand', 'twenty']
print(classification_report(val_data_gen.classes, y_pred, target_names=target_names))

# Compute confusion matrix
cnf_matrix = cm
np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=target_names,
                      title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

